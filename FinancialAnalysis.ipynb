{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FinancialAnalysis.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPMJVoVNE9CYRbjjkBLa+Fb"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Extract Twitter Dataset with API\n",
        "\n",
        "Twitter offers the past seven days of data on their free API tier, so we will go back in 60-minute windows and extract ~100 tweets from within each of these windows. Requires bearer tokens as from registration process with Twitter for developer access."
      ],
      "metadata": {
        "id": "ZtfboBIx3sg5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nqfUR0P12Ml6"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime, timedelta\n",
        "import requests\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('bearer_token.txt') as fp:\n",
        "    BEARER_TOKEN = fp.read()"
      ],
      "metadata": {
        "id": "N9snpVG037y9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "endpoint = 'https://api.twitter.com/2/tweets/search/recent'\n",
        "headers = {'authorization': f'Bearer {BEARER_TOKEN}'}\n",
        "params = {\n",
        "    'query': '(amazon OR aws OR jeff bezos) (lang:en)',\n",
        "    'max_results': '100',\n",
        "    'tweet.fields': 'created_at, lang'}"
      ],
      "metadata": {
        "id": "RQGqeGog38nd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dtformat = '%Y-%m-%dT%H:%M:%SZ'"
      ],
      "metadata": {
        "id": "3wdzCcAQ3_44"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def time_periods(now, mins):\n",
        "    now = datetime.strptime(now, dtformat)\n",
        "    intervals_time = now - timedelta(minutes=mins) # time series at mins intervals\n",
        "    return intervals_time.strftime(dtformat)"
      ],
      "metadata": {
        "id": "ldE7kOf34HxJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "now = datetime.now()  # get the current datetime, this is our starting point\n",
        "last_week = now - timedelta(days=7)  # datetime one week ago = the finish line\n",
        "now = now.strftime(dtformat)  # convert now datetime to format for API"
      ],
      "metadata": {
        "id": "4yd5CqhO5Qo_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame()"
      ],
      "metadata": {
        "id": "B9hX5I3O5hiN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def twitter_data(tweet):\n",
        "    data = {\n",
        "        'id': tweet['id_str'],\n",
        "        'created_at': tweet['created_at'],\n",
        "        'text': tweet['full_text']\n",
        "    }\n",
        "    return data"
      ],
      "metadata": {
        "id": "kp9T0lb154l9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "    if datetime.strptime(now, dtformat) < last_week:\n",
        "        break # loop based on earliest time\n",
        "      \n",
        "    pre60 = time_periods(now, 60)\n",
        "    params['start_time'] = pre60\n",
        "    params['end_time'] = now\n",
        "    response = requests.get(endpoint,\n",
        "                            params=params,\n",
        "                            headers=headers)\n",
        "    now = pre60\n",
        "\n",
        "    for tweet in response.json()['data']:\n",
        "        row = twitter_data(tweet)\n",
        "        df = df.append(row, ignore_index=True)"
      ],
      "metadata": {
        "id": "5BMaXIIZ5jEz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Beautiful Soup\n",
        "\n",
        "Extraction from websites based on html format - e.g. see code referring to find_all by class. Understand structure of website and develop code for that."
      ],
      "metadata": {
        "id": "BdjuDk6ZWVR0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# FOR REFERENCE\n",
        "seed_urls = ['https://inshorts.com/en/read/financial']\n",
        "\n",
        "def build_dataset(seed_urls):\n",
        "    news_data = []\n",
        "    for url in seed_urls:\n",
        "        news_category = url.split('/')[-1]\n",
        "        data = requests.get(url)\n",
        "        soup = BeautifulSoup(data.content, 'html.parser')\n",
        "        \n",
        "        news_articles = [{'news_headline': headline.find('span', \n",
        "                                                         attrs={\"itemprop\": \"headline\"}).string,\n",
        "                          'news_article': article.find('div', \n",
        "                                                       attrs={\"itemprop\": \"articleBody\"}).string,\n",
        "                          'news_category': news_category}\n",
        "                         \n",
        "                            for headline, article in \n",
        "                             zip(soup.find_all('div', \n",
        "                                               class_=[\"news-card-title news-right-box\"]),\n",
        "                                 soup.find_all('div', \n",
        "                                               class_=[\"news-card-content news-right-box\"]))\n",
        "                        ]\n",
        "        news_data.extend(news_articles)\n",
        "        \n",
        "    df =  pd.DataFrame(news_data)\n",
        "    df = df[['news_headline', 'news_article', 'news_category']]\n",
        "    return df"
      ],
      "metadata": {
        "id": "kLi9RqXXWUnn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gathering Financial News / Sentiment Analysis with BERT\n",
        "\n",
        "Historical analyst headlines and financial news headlines over several years on a sample of listed companies. Sentiment analysis using flair; classification into positive / negative with probability."
      ],
      "metadata": {
        "id": "MGjRAMPl-XbC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tFi8tvBp-ah_",
        "outputId": "1e929c3f-14fb-4bb9-8ace-d78e7b146bc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "root_folder='/content/drive/My Drive/DataAnalysis'"
      ],
      "metadata": {
        "id": "eM7qcZRk-ukO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(root_folder+'/analyst_ratings_processed.csv')\n",
        "data.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jJU72hoM-wG0",
        "outputId": "2739efcd-6b22-4a9b-8b6a-ca736e58ddd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1400469 entries, 0 to 1400468\n",
            "Data columns (total 4 columns):\n",
            " #   Column      Non-Null Count    Dtype  \n",
            "---  ------      --------------    -----  \n",
            " 0   Unnamed: 0  1399180 non-null  float64\n",
            " 1   title       1400469 non-null  object \n",
            " 2   date        1399180 non-null  object \n",
            " 3   stock       1397891 non-null  object \n",
            "dtypes: float64(1), object(3)\n",
            "memory usage: 42.7+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = data.dropna(axis=0)"
      ],
      "metadata": {
        "id": "MG5oPhfSEZdx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['date'] = data['date'].str.split(' ').str[0]\n",
        "data['date'] = pd.Series(pd.to_datetime(data['date'], format='%Y-%m-%d')) # 2020-06-05 10:30:00-04:00"
      ],
      "metadata": {
        "id": "Oo5PU8G--0g7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = data.drop('Unnamed: 0', axis=1)"
      ],
      "metadata": {
        "id": "9whjL_wOE1Rf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Flair / DistilBERT\n",
        "\n",
        "This model splits the text into character-level tokens and uses the DistilBERT model to make predictions.The advantage of working at the character-level (as opposed to word-level) is that words that the network has never seen before can still be assigned a sentiment. DistilBERT is a distilled version of the powerful BERT transformer model"
      ],
      "metadata": {
        "id": "X6WNF9WKIhmT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import flair\n",
        "sentiment_model = flair.models.TextClassifier.load('en-sentiment')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o5upNGYmGy1g",
        "outputId": "52824632-1986-458c-a920-29faf050e359"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-02-20 22:01:09,439 loading file /root/.flair/models/sentiment-en-mix-distillbert_4.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = data.sample(n = 1000, replace = False)\n",
        "data['text_clean'] = data['title'].apply(lambda x : text_cleaner(x))"
      ],
      "metadata": {
        "id": "b0annuv0HURZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['text_obj'] = data['text_clean'].apply(lambda x : flair.data.Sentence(x))\n",
        "data['text_obj'].apply(lambda x : sentiment_model.predict(x))\n",
        "data['score'] = data['text_obj'].apply(lambda x : x.labels[0].score)\n",
        "data['value'] = data['text_obj'].apply(lambda x : x.labels[0].value)"
      ],
      "metadata": {
        "id": "anCXBuwRHpTk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 832
        },
        "id": "2TJ3n2RiLPs6",
        "outputId": "697a88ea-afae-4f20-f47c-fe28326727ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-2bb1f8b3-350c-4f68-bba5-7503fcfe6298\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>date</th>\n",
              "      <th>stock</th>\n",
              "      <th>text_clean</th>\n",
              "      <th>text_obj</th>\n",
              "      <th>score</th>\n",
              "      <th>value</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>336161</th>\n",
              "      <td>Diebold's Planned Acquisition Of Wincor Nixdor...</td>\n",
              "      <td>2016-05-31</td>\n",
              "      <td>DBD</td>\n",
              "      <td>diebold s planned acquisition of wincor nixdor...</td>\n",
              "      <td>(Token: 1 diebold, Token: 2 s, Token: 3 planne...</td>\n",
              "      <td>0.556393</td>\n",
              "      <td>POSITIVE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>570059</th>\n",
              "      <td>Atlantic Equities Downgrades Garmin to Neutral...</td>\n",
              "      <td>2015-02-20</td>\n",
              "      <td>GRMN</td>\n",
              "      <td>atlantic equities downgrades garmin to neutral...</td>\n",
              "      <td>(Token: 1 atlantic, Token: 2 equities, Token: ...</td>\n",
              "      <td>0.998690</td>\n",
              "      <td>NEGATIVE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1264993</th>\n",
              "      <td>TherapeuticsMD Reports Resubmission Of NDA For...</td>\n",
              "      <td>2017-11-29</td>\n",
              "      <td>TXMD</td>\n",
              "      <td>therapeuticsmd reports resubmission of nda for...</td>\n",
              "      <td>(Token: 1 therapeuticsmd, Token: 2 reports, To...</td>\n",
              "      <td>0.984767</td>\n",
              "      <td>POSITIVE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>478804</th>\n",
              "      <td>FAA To Streamline Fire Regulations For Cargo C...</td>\n",
              "      <td>2019-07-03</td>\n",
              "      <td>FDX</td>\n",
              "      <td>faa to streamline fire regulations for cargo c...</td>\n",
              "      <td>(Token: 1 faa, Token: 2 to, Token: 3 streamlin...</td>\n",
              "      <td>0.790327</td>\n",
              "      <td>POSITIVE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1232600</th>\n",
              "      <td>Earnings Scheduled For February 25, 2014</td>\n",
              "      <td>2014-02-25</td>\n",
              "      <td>TOL</td>\n",
              "      <td>earnings scheduled for february</td>\n",
              "      <td>(Token: 1 earnings, Token: 2 scheduled, Token:...</td>\n",
              "      <td>0.872605</td>\n",
              "      <td>POSITIVE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1036042</th>\n",
              "      <td>American Petro-Hunter's Sacramento Gas Project...</td>\n",
              "      <td>2009-08-10</td>\n",
              "      <td>Q</td>\n",
              "      <td>american petro hunter s sacramento gas project...</td>\n",
              "      <td>(Token: 1 american, Token: 2 petro, Token: 3 h...</td>\n",
              "      <td>0.612436</td>\n",
              "      <td>POSITIVE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1119241</th>\n",
              "      <td>Morning Market Losers</td>\n",
              "      <td>2013-06-10</td>\n",
              "      <td>SHI</td>\n",
              "      <td>morning market losers</td>\n",
              "      <td>(Token: 1 morning, Token: 2 market, Token: 3 l...</td>\n",
              "      <td>0.999858</td>\n",
              "      <td>NEGATIVE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>555722</th>\n",
              "      <td>Mid-Morning Market Update: Markets Open Lower;...</td>\n",
              "      <td>2016-06-10</td>\n",
              "      <td>GNW</td>\n",
              "      <td>mid morning market update  markets open lower ...</td>\n",
              "      <td>(Token: 1 mid, Token: 2 morning, Token: 3 mark...</td>\n",
              "      <td>0.995033</td>\n",
              "      <td>NEGATIVE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>137174</th>\n",
              "      <td>Shares of several Brazilian bank stocks tradin...</td>\n",
              "      <td>2019-03-28</td>\n",
              "      <td>BBD</td>\n",
              "      <td>shares of several brazilian bank stocks tradin...</td>\n",
              "      <td>(Token: 1 shares, Token: 2 of, Token: 3 severa...</td>\n",
              "      <td>0.992516</td>\n",
              "      <td>POSITIVE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13092</th>\n",
              "      <td>Stocks Which Set New 52-Week Low Yesterday, Tu...</td>\n",
              "      <td>2018-12-20</td>\n",
              "      <td>ACM</td>\n",
              "      <td>stocks which set new    week low yesterday  tu...</td>\n",
              "      <td>(Token: 1 stocks, Token: 2 which, Token: 3 set...</td>\n",
              "      <td>0.999571</td>\n",
              "      <td>NEGATIVE</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows × 7 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2bb1f8b3-350c-4f68-bba5-7503fcfe6298')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2bb1f8b3-350c-4f68-bba5-7503fcfe6298 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2bb1f8b3-350c-4f68-bba5-7503fcfe6298');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                                     title  ...     value\n",
              "336161   Diebold's Planned Acquisition Of Wincor Nixdor...  ...  POSITIVE\n",
              "570059   Atlantic Equities Downgrades Garmin to Neutral...  ...  NEGATIVE\n",
              "1264993  TherapeuticsMD Reports Resubmission Of NDA For...  ...  POSITIVE\n",
              "478804   FAA To Streamline Fire Regulations For Cargo C...  ...  POSITIVE\n",
              "1232600           Earnings Scheduled For February 25, 2014  ...  POSITIVE\n",
              "...                                                    ...  ...       ...\n",
              "1036042  American Petro-Hunter's Sacramento Gas Project...  ...  POSITIVE\n",
              "1119241                             Morning Market Losers   ...  NEGATIVE\n",
              "555722   Mid-Morning Market Update: Markets Open Lower;...  ...  NEGATIVE\n",
              "137174   Shares of several Brazilian bank stocks tradin...  ...  POSITIVE\n",
              "13092    Stocks Which Set New 52-Week Low Yesterday, Tu...  ...  NEGATIVE\n",
              "\n",
              "[1000 rows x 7 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FinBERT\n",
        "\n",
        "*   Original BERT training Data English Wikipedia and BookCorpus (Zhu et al., 2015)\n",
        "*   Finance Articles from Yahoo Finance\n",
        "*   Financial News from Financial Web\n",
        "*   Question-Answer pairs about financial issues from Reddit"
      ],
      "metadata": {
        "id": "iLYuuZoyVt5R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "\n",
        "finbert = BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-tone',num_labels=3)\n",
        "tokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-tone')"
      ],
      "metadata": {
        "id": "k4bn2uwmUe5G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = {0:'neutral', 1:'positive',2:'negative'}"
      ],
      "metadata": {
        "id": "HERK-dZDUjxQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['text_obj_finBERT'] = data['text_clean'].apply(lambda x : tokenizer(x, return_tensors=\"pt\", padding=True))\n",
        "data['text_obj_finBERT'].apply(lambda x : finbert(**inputs)[0])\n",
        "data['sentiment_finBERT'] = data['text_obj_finBERT'].apply(lambda x : labels[np.argmax(x.detach().numpy())]"
      ],
      "metadata": {
        "id": "nfgNNwJOUlNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Financial News Analysis\n",
        "\n",
        "**Text-wranging and pre-processing.** Approached from a detailed perspective in contraction mapping, lemmatisation and stemming. Aim to standardise corpus and minimise vocabulary size for deep learning models or extractive techniques."
      ],
      "metadata": {
        "id": "u7tBTH9U6LPi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
        "                           \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
        "                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
        "                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
        "                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
        "                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
        "                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
        "                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n",
        "                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
        "                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
        "                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
        "                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
        "                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
        "                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
        "                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
        "                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
        "                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
        "                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
        "                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
        "                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
        "                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
        "                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
        "                           \"you're\": \"you are\", \"you've\": \"you have\"}"
      ],
      "metadata": {
        "id": "wqrgdw9mLh3U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def text_cleaner(text):\n",
        "    newString = text.lower()\n",
        "    newString = re.sub('\"', '', newString)\n",
        "    newString = re.sub(r'\\([^)]*\\)', '', newString)\n",
        "    newString = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in newString.split(' ')])\n",
        "    newString = re.sub(r\"(?i)http(s):\\/\\/[a-z0-9.~_\\-\\/]+\", '', newString)\n",
        "    newString = re.sub(r\"(?i)@[a-z0-9_]+\", '', newString)\n",
        "    newString = re.sub('[^a-zA-Z\\s]', ' ', newString)\n",
        "\n",
        "    return newString"
      ],
      "metadata": {
        "id": "cqXpi1we6qrb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "General cleaning technique above based on regex. e.g. r\"(?i)http(s):\\/\\/[a-z0-9.~_\\-\\/]+\" removes all such special characters. And final cleaning function to remove all non alpha-numerical characters. Only requires care in certain special characters needing to be replaced by space (e.g. \"-\") and others by BLANK."
      ],
      "metadata": {
        "id": "ciqxdrwiXgHp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def strip_html_tags(text):\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    stripped_text = soup.get_text()\n",
        "    return stripped_text"
      ],
      "metadata": {
        "id": "JLV7xmyaW7sq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_accented_chars(text):\n",
        "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "    return text"
      ],
      "metadata": {
        "id": "hZ9FMqr1XdlL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Word stems** are also known as the base form of a word, and we can create new words by attaching affixes to them in a process known as inflection."
      ],
      "metadata": {
        "id": "I2sL-m1WYJ4S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def simple_stemmer(text):\n",
        "    ps = nltk.porter.PorterStemmer()\n",
        "    text = ' '.join([ps.stem(word) for word in text.split()])\n",
        "    return text"
      ],
      "metadata": {
        "id": "DufvwMFfYLEh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lemmatization** is very similar to stemming, where we remove word affixes to get to the base form of a word. However, the base form in this case is known as the root word, but not the root stem. The difference being that the root word is always a lexicographically correct word"
      ],
      "metadata": {
        "id": "5eBoT2xwYNFf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmatize_text(text):\n",
        "    text = nlp(text)\n",
        "    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n",
        "    return text"
      ],
      "metadata": {
        "id": "bT0Y3qDCYUoU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stopwords** - typically, these can be articles, conjunctions, prepositions and so on. Some examples of stopwords are a, an, the, and the like."
      ],
      "metadata": {
        "id": "oTo8FNFUYcF1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stopwords_shortwords(text, length):\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    tokens = [token.strip() for token in tokens]\n",
        "    filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
        "\n",
        "    long_words = []\n",
        "    prev_word = []\n",
        "    for i in filtered_tokens:\n",
        "        if i not in prev_word and len(i) >= length:\n",
        "            long_words.append(i)\n",
        "            prev_word = [i]    \n",
        "    return (\" \".join(long_words)).strip()"
      ],
      "metadata": {
        "id": "7bFbelYXYWOz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**General function** to combine preprocessing steps."
      ],
      "metadata": {
        "id": "z9y-eHQMZ4HJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_corpus(corpus, html_stripping=True,\n",
        "                     accented_char_removal=True, text_lower_case=True, \n",
        "                     text_lemmatization=True, simple_stemmer=True, \n",
        "                     remove_stopwords_shortwords=True):\n",
        "    \n",
        "    normalized_corpus = []\n",
        "\n",
        "    for doc in corpus:\n",
        "        if html_stripping:\n",
        "            doc = strip_html_tags(doc)\n",
        "        if accented_char_removal:\n",
        "            doc = remove_accented_chars(doc)  \n",
        "        if text_lower_case:\n",
        "            doc = doc.lower()\n",
        "        if text_lemmatization:\n",
        "            doc = lemmatize_text(doc)\n",
        "        if simple_stemmer:\n",
        "            doc = simple_stemmer(doc)\n",
        "        if remove_stopwords_shortwords:\n",
        "            doc = remove_stopwords_shortwords(doc)\n",
        "        # remove extra newlines\n",
        "        doc = re.sub(r'[\\r|\\n|\\r\\n]+', ' ',doc)\n",
        "        normalized_corpus.append(doc)\n",
        "    \n",
        "    return normalized_corpus"
      ],
      "metadata": {
        "id": "RojaBZ6XZCnO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Language Syntax and Structure"
      ],
      "metadata": {
        "id": "7LFaX9BoaEhk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**N(oun)**: This usually denotes words that depict some object or entity, which may be living or nonliving.\n",
        "\n",
        "**V(erb)**: Verbs are words that are used to describe certain actions, states, or occurrences. There are a wide variety of further subcategories, such as auxiliary, reflexive, and transitive verbs (and many more).\n",
        "\n",
        "**Adj(ective)**: Adjectives are words used to describe or qualify other words, typically nouns and noun phrases. The POS tag symbol for adjectives is ADJ.\n",
        "\n",
        "**Adv(erb)**: Adverbs usually act as modifiers for other words including nouns, adjectives, verbs, or other adverbs. The phrase very beautiful flower has the adverb (ADV) very, which modifies the adjective (ADJ) beautiful , indicating the degree to which the flower is beautiful. The POS tag symbol for adverbs is ADV.\n",
        "\n",
        "Further include pronouns, prepositions, interjections, conjunctions, determiners, and many others. Plus each POS tag like the noun (N) can be further subdivided into categories like singular nouns (NN), singular proper nouns (NNP), and plural nouns (NNS)."
      ],
      "metadata": {
        "id": "0MOsz_RLaKEM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "t5KI7aCZZ3af"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}